"""
Sistema de evaluaci√≥n de c√≥digo para cada challenge.
A√±ade aqu√≠ la l√≥gica espec√≠fica para evaluar cada ejercicio.
"""

import time
import sys
from io import StringIO
import traceback


class CodeEvaluator:
    """
    Clase base para evaluar c√≥digo de submissions.
    """

    @staticmethod
    def evaluate_challenge_35(code: str) -> tuple[int, bool, str, float]:
        """
        Eval√∫a el Challenge 35: A Halloween Carol - Quantum Chemistry Mystery

        Returns:
            tuple: (score, passed, feedback, execution_time)
        """
        start_time = time.time()

        try:
            # Crear un namespace para ejecutar el c√≥digo
            local_scope = {}

            # Ejecutar el c√≥digo del usuario
            exec(code, {}, local_scope)

            # Verificar que existan las funciones/variables requeridas
            required_items = [
                'alpha_vqe_result',
                'beta_vqe_result',
                'alpha_gap_ev',
                'beta_gap_ev',
                'alpha_homo_lumo',
                'beta_homo_lumo'
            ]

            missing = [item for item in required_items if item not in local_scope]

            if missing:
                feedback = f"‚ùå Faltan variables/resultados: {', '.join(missing)}\n"
                feedback += "Aseg√∫rate de ejecutar todo el notebook y tener todos los resultados."
                return 0, False, feedback, time.time() - start_time

            # Extraer resultados
            alpha_vqe = local_scope.get('alpha_vqe_result')
            beta_vqe = local_scope.get('beta_vqe_result')
            alpha_gap = local_scope.get('alpha_gap_ev')
            beta_gap = local_scope.get('beta_gap_ev')
            alpha_homo_lumo = local_scope.get('alpha_homo_lumo')
            beta_homo_lumo = local_scope.get('beta_homo_lumo')

            # Sistema de puntuaci√≥n
            score = 0
            feedback_parts = []

            # Task 1: VQE Analysis (30 puntos)
            if alpha_vqe is not None and beta_vqe is not None:
                # Verificar que los resultados sean razonables
                try:
                    alpha_energy = float(alpha_vqe.eigenvalue) if hasattr(alpha_vqe, 'eigenvalue') else float(alpha_vqe)
                    beta_energy = float(beta_vqe.eigenvalue) if hasattr(beta_vqe, 'eigenvalue') else float(beta_vqe)

                    # Los valores deben estar en un rango razonable para qu√≠mica cu√°ntica
                    if -10 < alpha_energy < 0 and -10 < beta_energy < 0:
                        score += 30
                        feedback_parts.append("‚úÖ Task 1 (VQE): Excelente! Energ√≠as calculadas correctamente.")
                    else:
                        score += 15
                        feedback_parts.append("‚ö†Ô∏è Task 1 (VQE): Energ√≠as calculadas pero parecen fuera de rango.")
                except:
                    score += 10
                    feedback_parts.append("‚ö†Ô∏è Task 1 (VQE): Resultados parciales.")
            else:
                feedback_parts.append("‚ùå Task 1 (VQE): No se encontraron resultados VQE.")

            # Task 2: HOMO-LUMO Gap (30 puntos)
            if alpha_gap is not None and beta_gap is not None:
                try:
                    alpha_gap_val = float(alpha_gap)
                    beta_gap_val = float(beta_gap)

                    # Los gaps deben ser positivos y razonables (0-10 eV t√≠picamente)
                    if 0 < alpha_gap_val < 15 and 0 < beta_gap_val < 15:
                        score += 30
                        feedback_parts.append(f"‚úÖ Task 2 (HOMO-LUMO): Perfecto! Alpha gap: {alpha_gap_val:.2f} eV, Beta gap: {beta_gap_val:.2f} eV")

                        # Bonus por interpretaci√≥n correcta
                        if beta_gap_val < alpha_gap_val:
                            feedback_parts.append("   üí° Beta es m√°s reactivo que Alpha (gap menor)")
                    else:
                        score += 15
                        feedback_parts.append("‚ö†Ô∏è Task 2 (HOMO-LUMO): Gaps calculados pero valores inusuales.")
                except:
                    score += 10
                    feedback_parts.append("‚ö†Ô∏è Task 2 (HOMO-LUMO): Error al procesar los gaps.")
            else:
                feedback_parts.append("‚ùå Task 2 (HOMO-LUMO): No se encontraron c√°lculos de gap.")

            # Task 3: QSD Analysis (40 puntos)
            # Este es el m√°s complejo, verificamos que al menos se haya intentado
            qsd_indicators = ['V_alpha', 'V_beta', 'evals_alpha', 'evals_beta', 'fidelity_matrix']
            qsd_present = sum(1 for item in qsd_indicators if item in local_scope)

            if qsd_present >= 3:
                score += 40
                feedback_parts.append("‚úÖ Task 3 (QSD): Excelente! An√°lisis de subespacio cu√°ntico completado.")
            elif qsd_present >= 2:
                score += 25
                feedback_parts.append("‚ö†Ô∏è Task 3 (QSD): An√°lisis parcial de QSD.")
            elif qsd_present >= 1:
                score += 15
                feedback_parts.append("‚ö†Ô∏è Task 3 (QSD): Inicio de an√°lisis QSD detectado.")
            else:
                feedback_parts.append("‚ùå Task 3 (QSD): No se encontr√≥ an√°lisis QSD.")

            # Determinar si pas√≥
            passed = score >= 70

            # Construir feedback final
            feedback = "\n".join(feedback_parts)
            feedback += f"\n\nüìä Puntuaci√≥n final: {score}/100"

            if passed:
                feedback += "\nüéÉ ¬°Felicidades! Has completado el Halloween Challenge."
            else:
                feedback += "\nüíÄ Sigue intentando. Revisa las tareas que faltan."

            execution_time = time.time() - start_time
            return score, passed, feedback, execution_time

        except SyntaxError as e:
            execution_time = time.time() - start_time
            return 0, False, f"‚ùå Error de sintaxis: {str(e)}", execution_time
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"‚ùå Error al evaluar: {str(e)}\n{traceback.format_exc()}"
            return 0, False, error_msg, execution_time

    @staticmethod
    def evaluate(challenge_id: int, code: str) -> tuple[int, bool, str, float]:
        """
        M√©todo principal para evaluar c√≥digo seg√∫n el challenge.

        Args:
            challenge_id: ID del challenge
            code: C√≥digo a evaluar

        Returns:
            tuple: (score, passed, feedback, execution_time)
        """
        evaluators = {
            35: CodeEvaluator.evaluate_challenge_35,
        }

        evaluator = evaluators.get(challenge_id)

        if not evaluator:
            return 0, False, f"‚ùå No hay evaluador para challenge {challenge_id}. Solo el Challenge 35 est√° disponible.", 0.0

        try:
            return evaluator(code)
        except Exception as e:
            return 0, False, f"‚ùå Error cr√≠tico en evaluaci√≥n: {str(e)}", 0.0
# Funci√≥n helper para capturar stdout durante la ejecuci√≥n
def capture_stdout(func, *args, **kwargs):
    """Captura el stdout durante la ejecuci√≥n de una funci√≥n"""
    old_stdout = sys.stdout
    sys.stdout = StringIO()

    try:
        result = func(*args, **kwargs)
        output = sys.stdout.getvalue()
        return result, output
    finally:
        sys.stdout = old_stdout
